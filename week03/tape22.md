# 3장 연결성 및 조합성 패턴

마이크로서비스 간의 연결성을 확보하는 패턴들과 <br>
service composition 패턴을 통해 여러 서비스를 통합하여 새 비즈니스 기능을 만드는 법에 대해 논한다.

---

### 서비스 연결성 패턴

- 마이크로 서비스 간 연결성을 확보하는 방법
- 동기/비동기와 같은 하나 이상의 통신 패턴으로 연결성을 만들어냄.
- 데이터베이스나 메시지 브로커 등 다양한 시스템과 연결되기도 하고
- API 게이트웨이를 사용해 사용자에게 제공할 비즈니스 기능들을 API gateway로 노출하기도 함
- 상황에 따라 RPC 패턴, 큐 기반 통신, 이벤트 스트림 같은 패턴들을 처리할 필요도 있음

<img width="512" height="556" alt="Screenshot 2025-09-25 at 11 51 41 AM" src="https://github.com/user-attachments/assets/95bb9c9c-ca2d-4c4b-98f4-3122e5e8eab5" />


- 쇼핑몰 애플리케이션을 예시로 들어보면, 각 서비스 간 연결에 적합한 통신 패턴들을 사용함
- 주문, 상품목록같은 외부로 제공되는 서비스는 REST, GraphQL
- 주문과 결제 서비스 간 통신같은 경우는 카프카 브로커를 통한 비동기 메시징 패턴

<img width="798" height="638" alt="Screenshot 2025-09-25 at 11 52 23 AM" src="https://github.com/user-attachments/assets/faf884ea-8ba4-4f94-b5df-402420caff86" />

- 마이크로서비스 시스템이 늘어날 수록 복잡도도 증가하기 때문에 서비스를 너무 잘게 나눈 것은 아닌지 고려하는 것이 중요함
- 또한 기본적인 통신 패턴은 비즈니스 기능 별로 결정해야함
    - 상품 검색 기능은 request-response 패턴,
    - 주문 생성과 처리는 비동기 단일 수신자 패턴 등
- 서비스 간 통신을 어떤 패턴으로 구현할지 파악하는데 시간이 걸리며 보통 설계 단계에서 이루어짐

---
<br>

### 서비스 추상화 패턴

- 구현이나 상호작용 대상, 배포 구조등에 대한 자세한 내용은 숨기기 위해 추상화를 사용
    - 추상화하지 않으면 서비스를 사용하는 모든 클라이언트나 서비스가 세세한 내용을 다 알아야지만 서비스를 사용할 수 있게 됨
- 장점
    - 추상화는 시스템 위치를 ip와 같이 고정하거나 안정적인 위치로 나타낼 수 있음
    - 서비스 레지스트리와 같은 검색 패턴으로 중앙에서 정보를 관리하면 세부구현을 몰라도 알 수 있으며,
    - 부하분산, 장애복구에 용이하고 인스턴스를 언제든지 생성/삭제할 수 있기 때문에 동적으로 크기 조절이 가능해짐
- **쿠버네티스**같은 플랫폼은 서비스 추상화 기능을 기본적으로 제공함
    - 쿠버네티스는 상품목록 파드들을 하나의 상품목록 서비스로 묶어서 추상화할 수 있음
    - 외부에서 호출할 수 있는 클러스터 내부IP를 할당받고, 클라이언트는 클러스터 내의 안정적인 IP주소로 요청을 보내며, 이 요청은 상품목록 파드 중 한 곳으로 전달됨
 <img width="781" height="504" alt="Screenshot 2025-09-25 at 11 53 12 AM" src="https://github.com/user-attachments/assets/9e54f7a3-12d2-4ba0-a886-1ea568b71aa3" />

- 또한 묶여있는 파드 간 부하분산 기능도 제공함
    - service 유형을 LoadBalancer로 지정하면, 네트워크 부하분산 서비스를 생성함
    - 외부에서 접근 가능한 IP주소를 가지며 요청을 받으면 적절한 노드의 포트로 요청을 분산해서 보냄
 
⇒ 서비스 추상화 패턴은 모놀리식 시스템 또한 추상화할 수 있는데, service 유형을 External-Name으로 지정하면 외부 DNS 이름을 내부에서 사용하는 별칭으로 만들어서 제공함

<br>

### 서비스 레지스트리 및 검색 패턴

- 서비스가 늘어나면서 정보들을 별도로 보관하고 관리할 필요가 있음
- 서비스 정보를 한데 모아서 관리하면 검색이 쉬워짐
- 이런 서비스 정보와 메타 데이터를 저장하는 스토어가 서비스 레지스트리
    - 서비스 URL, 오픈 API나 gRPC protobuf와 같은 서비스 인터페이스 정의 등이 포함됨
    - 각 서비스를 정규화된 표현방식으로 나타내기 때문에 서비스 구현 기술과 상관없이 메타데이터를 저장하고 정의할 수 있다는 장점
- 서비스 레지스트리는 스토어 API, 검색 API를 제공하며,
- client-side 서비스 디스커버리와 server-side 디스커버리 2가지 방법으로 구현할 수 있음

<br>

- client-side는 클라이언트가 서비스를 검색하고 사용
    - 사용자가 서비스 정보를 검색/발견하면 → 관련 서비스를 통해 서비스를 호출하는 구조
    - 개발 단계에서는 서비스 규약과 정보를 얻어서 애플리케이션을 이에 맞게 개발
<img width="529" height="195" alt="Screenshot 2025-09-25 at 11 56 22 AM" src="https://github.com/user-attachments/assets/058b031d-da4e-4cf8-b2b3-fd3aa2975211" />

- server-side 디스커버리는 로드밸런서 같은 컴포넌트에서 서비스를 검색하는 방식
    - 서비스 요청이 들어오면 로드밸런서가 레지스트리로부터 URL 같은 정보를 파악하고 서비스를 호출
    - 매번 레지스트리에 접근하지 않고 주로 로드밸런서 캐시에 서비스 정보를 저장하고 사용함
 
<img width="535" height="228" alt="Screenshot 2025-09-25 at 11 56 35 AM" src="https://github.com/user-attachments/assets/22912824-dc0d-4c07-a7ce-f1259d84113d" />

- 서비스 배포 과정에서 모든 마이크로 서비스를 서비스 레지스트리에 등록하며,
- 서비스가 주기적으로 레지스트리에 하트비트를 보내 서비스 상태를 검사

<img width="527" height="263" alt="Screenshot 2025-09-25 at 11 56 53 AM" src="https://github.com/user-attachments/assets/2293d7d1-2c64-46a1-b45b-61711364c527" />

- 쿠버네티스의 경우 파드 검색을 위해 DNS 이름을 기본적으로 사용하는데, 내부적으로 etcd라는 분산 key-value 스토어를 서비스 레지스트리처럼 사용함
- consul과 같은 서비스 레지스트리 레벨의 메타데이터 관리 기능을 제공하지는 않아서, 쿠버네티스의 경우 별도의 서비스 레지스트리를 사용하는 것이 좋음

- 서비스 레지스트리의 경우 초기부터 반드시 만들어야하는 것은 아니고, 애플리케이션을 개발하면서 서비스 간의 의존성 등 정보 관리가 복잡해지거나 리더 선출과 같은 추가 기능이 필요해지는 시점에 도입해도 늦지 않음



### 탄력적인 연결 패턴 (resilience connectivity pattern)

- 마이크로 서비스 간 연결을 할 때는 네트워크를 사용하는데,
- 분산 컴퓨팅 분야에서 보통 네트워크는 신뢰할 수 없는 영역임
- 그래서 마이크로서비스 시스템을 연결할 때는 반드시 탄력적인 연결성 기법을 사용해야함

- resilience connectivity pattern은 마이크로 서비스 간의 연결에 생기는 문제를 처리하거나 복구하는 패턴
- 서비스 런타임의 일부로서 탄력적인 통신 로직을 구성해야한다는 점

<img width="525" height="108" alt="Screenshot 2025-09-25 at 11 57 32 AM" src="https://github.com/user-attachments/assets/d57bc6ec-04cc-477d-9888-cadb72386566" />

- timeout
    - 서비스를 호출하고 응답을 기다리는 시간
    - 다른 시스템 호출 시 타임아웃을 지정하지 않으면 계속 기다리면서 서비스에 영향을 줄 수 있음
    - 타임아웃을 설정해서 지정한 시간 내 응답이 오지 않았을 때를 처리할 수 있음
- retry
    - 네트워크 통신은 간헐적으로 오류가 발생할 수도 있으므로
    - 같은 상황에서 동일한 서비스를 한 번 이상 반복 호출하여 응답을 얻음
    - 어느 정도의 간격을 두어서 최대 몇 번의 retry 를 할 지를 지정할 수 있음
    - backoff / jitter
- deadliine
    - 서비스 호출이 완료되어야하는 고정 시간
    - 클라이언트가 요청을 보내는 시간에 요청 데드라인을 지정하고, 서비스 스트림을 따라 연계되는 서비스로 전파됨
    - 시간을 초과하면 별도의 처리로직을 호출해야함
    - 상품관리 → 재고서비스 호출과 같은 케이스
<img width="497" height="146" alt="Screenshot 2025-09-25 at 11 57 44 AM" src="https://github.com/user-attachments/assets/68e4bc0a-5511-49b4-993e-883b5e443c1d" />


- circuit breaker
    - 호출한 서비스가 계속 실패하는 경우, 다시 호출하면 시스템에 외부 서비스의 장애가 전파됨
    - 서비스 호출 실패 횟수가 특정 임계치를 넘으면 서비스에 대한 호출을 막는 로직
    - 정상일 때는 closed 상태이며, 임계치를 넘으면 서킷을 open 상태로 변경하여 서비스 호출을 차단함
        - circuit reset timeout 은 일정 기간동안 open이 유지되면 closed 처리하고 서비스 호출을 다시 시도하는 초기화 동작
        - 지정된 시간동안 open 상태로 있으면 half-open 상태로 바꾸고 시험삼아 호출을 허용하기도 함
    - 외부 시스템 장애 시 추가적인 피해나 오류가 전파되는 것을 방지함
    - backoff 기법, timeout, 초기화 간격, 에러코드 등 설정이 다양함

- fail-fast
    - 빠르게 실패 응답을 받는 것이 더 좋다.
    - 서비스 연결성과 관련된 비정상적인 동작이나 오류를 최대한 빨리 탐지하는 것이 fail-fast의 핵심
    - 요청을 보내기 전 검증을 먼저 하면서 실제 서비스, 시스템을 호출하지 않고도 문제 상황을 탐지

- 탄력적 연결성은 클라우드 네이티브 애플리케이션의 핵심 요소이며, 라이브러리 등을 통해 구현하거나, 사이드카로 런타임을 통해 구성하거나, 클라우드 플랫폼에 제공하는 기본 기능을 사용하기도 함


---

<br>

### 사이드카  패턴

- 마이크로서비스 곁에 또 다른 마이크로 서비스 컨테이너를 함께 쓰는 방식
- 주 컨테이너의 기능을 확장하거나 향상할 때 사이드카 컨테이너를 사용
- 사이드카 패턴은 마이크로서비스 런타임과 별개로 다른 런타임에 서비스 간 통신 로직을 구현할 수 있음

<img width="489" height="145" alt="Screenshot 2025-09-25 at 11 58 22 AM" src="https://github.com/user-attachments/assets/3db6994d-d87e-4969-bfab-39e5fccc343a" />

    - 각 서비스의 비즈니스 로직에서는 기저 네트워크 통신에 대해 신경쓰지 않아도 되며,
    - 사이드카가 외부와 연결되는 통신을 책임지기 때문에 주 컨테이너는 다른 서비스나 시스템과 직접 통신하지 않아도 됨


- 쿠버네티스 환경에서는 사이드카 컨테이너와 함께 배포하는 것을 멀티 컨테이너 파드라고 부름
    - proxy처럼 사용해서 통신을 관리할 수 있음
        - 주 컨테이너는 외부 서비스나 시스템 호출을 위해 localhost의 사이드카를 호출함
        - 보안 / 서비스 검색 등 추가적인 네트워크 통신 기능을 더해서 외부로 요청을 보내는 프록시 역할
        - envoy 라는 사이드카 프록시를 통해 http 프로토콜로 통신
        - envoy가 제공하는 API를 통해 프록시 동작을 조절하거나, mongoDB sniffing filter을 적용해서 서비스와 DB간의 통신을 모니터링하고 다른 모니터링 도구에 전송도 가능

<img width="493" height="247" alt="Screenshot 2025-09-25 at 11 59 00 AM" src="https://github.com/user-attachments/assets/c95a9e9d-687d-4347-adc3-c2fe465b95cc" />

<br>

- bridge
    - 사이드카 프록시 방식은 내부나 외부로 들어오고 나가는 프로토콜을 변경하지는 않음 (주 컨테이너와 외부 서비스를 연결해주는 역할)
    - 브릿지 기법은 두 개의 서로 다른 프로토콜을 연결할 수 있다.
        - 주 컨테이너는 http만 허락하지만 카프카와 같은 메시징 시스템에도 접근하고 싶을 때, 브릿지를 통해 연결함
        - Dapr 같은 사이드카를 사용하면 http를 통해 사이드카 API를 호출해서 카프카 메시지를 생성하고 소비할 수 있음.

<img width="519" height="232" alt="Screenshot 2025-09-25 at 11 59 17 AM" src="https://github.com/user-attachments/assets/449984ce-84b5-4e3b-a624-6e1952377ebb" />

- 사이드카 패턴은 주 컨테이너의 기능을 향상시켜줄 수 있지만,
    - 그만큼 인스턴스 수가 늘어나기 때문에 관리 포인트가 증가하고, 사이드카 설정 자체가 아주 복잡한 로직이 될 수 있음
    - 사이드카 컨테이너 관리는 별도의 컨트롤 플레인 컴포넌트에 의해 이루어져야함
    - 비즈니스 로직과 절대 섞이지 않도록 해야함.


<br>

### 서비스 메시 패턴(service mesh pattern)

- 사이드카 패턴을 확장한 것으로, 클라우드 네이티브 애플리케이션의 통신 인프라스트럭쳐에서 주로 사용됨
- EBS와 비교했을 때 마이크로서비스 아키텍처의 경우 내부 통신로직이 각 서비스 별로 구현되어야하며, 비즈니스 로직과 연결성 로직 모두 책임지고 관리해야하는 이슈가 생김
- 그래서 네트워크 통신 로직과 비즈니스 로직을 분리하기 위해 resilience4j와 같은 외부 라이브러리로 구현하기도 함

<img width="543" height="274" alt="Screenshot 2025-09-25 at 11 59 42 AM" src="https://github.com/user-attachments/assets/da41b383-c234-43f0-9441-a377a401db6b" />

- 서비스 간 통신 기능은 서로 다른 마이크로서비스에서도 요구사항이 비슷하므로 사이드카와 같은 다른 계층에 이런 기능들을 구현해서 서비스 코드를 독립적으로 유지할 수 있다.
    - 이것이 바로 서비스 메시 패턴의 핵심

- 서비스 메시 패턴으로 마이크로 서비스 간 내부 통신 인프라스트럭처를 만들 수 있으며, 마이크로 서비스 끼리는 직접 통신하지 않으며, 사이드카 프록시를 통해 이루어짐
- 서비스 메시 사이드카 프록시는 다음과 같은 컴포넌트를 사용함
    - 데이터 플레인 : 서비스/시스템 간 주고받는 데이터나 메시지에 내부 통신 로직이 적용됨
    - 컨트롤 플레인 : 사이드카 프록시는 컨트롤 플레인을 통해 제어
        - 중앙집중화된 컴포넌트는 API를 통해 데이터플레인에 위치한 사이드카 프록시를 제어
    - 서비스 메시 설정 언어 : 데이터 플레인 설정을 통해 통신 로직을 제어할 수 있는 설정 API 제공
    - 내장 기능들 : 신뢰성, 보안, 관측 가능성, 검색, 정책 적용 등 부가기능을 제공함
 

⇒ 개발자는 마이크로 서비스를 만들고 사이드카와 함께 배포(sidecar injection)하기만 하면 외부 시스템과 비즈니스 로직을 연결해줌.

- 사이드카와 주 컨테이너는 쿠버네티스 파드와 같이 하나의 단위로 배포됨
- 서비스 메시는 설정에 사용될 언어나 API를 통해 데이터 플레인을 관리하고, 기능을 제어함
    - 컨트롤 플레인으는 중앙에서 모든 사이드카를 연결하고 제어할 수 있게 해줌
    - 데이터 플레인은 신뢰성, 보안, 관측 가능성, 서비스 검색, 정책 적용 등의 기능을 제공
 <img width="550" height="237" alt="Screenshot 2025-09-25 at 12 00 27 PM" src="https://github.com/user-attachments/assets/6c2b0062-da59-4737-b0f4-8ac3ef3ae480" />

- 서비스 메시는 마이크로서비스 수가 증가함에 따라 배포와 관리가 점점 어렵고 복잡해질 때 사용함
    - 쿠버네티스같은 오케스트레이션 계층 위에 구성하여 서비스나 파드같은 추상화 개념을 통해 컨테이너 관리 부담을 덜 수 있음
    - Istio나 Linkerd 등이 주로 사용됨
    - 위 그림에서는 이중 Istio 구조를 나타내고 있으며,
    - 마이크로 서비스에 Istio 사이드카를 주입하면 프록시가 시스템 간 모든 네트워크 통신을 가로채어 관리하게됨
    - 그리고 컨트롤 플레인을 통해 통신을 설정하고 관리할수도 있음

<img width="531" height="266" alt="Screenshot 2025-09-25 at 12 00 40 PM" src="https://github.com/user-attachments/assets/79dcc8ee-825a-4d29-a7ff-6749ac8dc2ee" />

- Istio는 다음과 같은 핵심 기능을 제공함
    - 자동 부하분산 : Http, gRPC, 웹소켓, TCP 트래픽에 대한 자동 부하분산 제공
    - 트래픽 제어 : 라우팅 규칙, retry, failover, fault injection 등의 기능 제공
    - 정책 적용 : 자유롭게 추가 및 제거할 수 있는 정책 계층과, 설정 API를 통해 접근제어, 속도 제한, 리소스 제한 등의 정책 기능 제공
    - 관측 가능성 : 지표, 로그, 클러스터 내 트래픽 추적 등에 대한 관측 가능성 기능 제공
    - 보안 : 클러스터 내 서비스 간 통신을 강력한 신원 기반 인증 및 권한 부여로 보호해줌


⇒ 서비스 메시 배포 관리는 생각보다 복잡할 수 있고, 모든 사이드카 프록시를 실행하고 관리하는 데는 성능 부하가 발생함. <br>
⇒ 또한 아직 비동기 이벤트 주도 통신같은 통신 패턴을 지원하지 않고 있기 때문에 도입 시 고려가 필요하다.


### 사이드카 없는 서비스 메시 패턴

인스턴스 별로 사이드카 프록시를 두어서 발생할 수 있는 부하를 해결하기 위한 패턴으로, 

- 컨트롤 플레인이 사이드카 프록시의 네트워크 통신을 관리하고 제어할 수 있다면
- 주 컨테이너의 컴포넌트를 바로 관리할 수도 있다는 생각에서 착안

<img width="544" height="174" alt="Screenshot 2025-09-25 at 12 01 14 PM" src="https://github.com/user-attachments/assets/9ae79b83-a37a-4fa4-b349-b04e8a2edb6c" />

- 서비스 메시 패턴과 비슷하게 메시 트래픽을 관리하고 설정할 컨트롤 플레인을 사용하되,
- 마이크로 서비스 런타임 자체에 사이드카 프록시 로직을 내장하여
- 내장 런타임은 컨트롤 플레인 통신 프로토콜을 통해 컨트롤 플레인으로부터 설정을 받는 구조.
- 사이드카 없이 마이크로서비스 런타임 자체만으로 중앙 컨트롤 플레인을 통해 서비스 간 통신 설정을 적용할 수 있으나
- 마이크로서비스 각 기술스택이 컨트롤 플레인 API와 네트워크 통신 계층 별로 요구하는 통신 로직을 지원해야한다는 단점이 있음

⇒ 이걸 실제 현업에서도 사용하는 패턴인지?

예) 구글 트래픽 디렉터의 gRPC 서비스

<img width="513" height="207" alt="Screenshot 2025-09-25 at 12 01 33 PM" src="https://github.com/user-attachments/assets/440693e6-cc35-4df7-a295-3fb7413cce3a" />

- envoy의 설정 API를 통해 사이드카 프록시를 제어하는 컨트롤 플레인 역할을 함
- gRPC 기반 마이크로서비스를 사이드카 없이 배포하면, gRPC 클라이언트에서 envoy의 설정 프로토콜인 xDS를 통해 컨트롤 플레인과 통신하여 설정을 전달받는다.
    - 트래픽 디렉터는 gRPC 클라이언트에 어떤 서비스와 연결하고, 어떻게 부하분산 요청을 보낼지 등을 알려줌
    - 클라이언트 입장에서는 envoy xDS API를 사용하면 사이드카 프록시 주입 없이도 설정값을 얻을 수 있다.

하지만 사이드카 없는 서비스 메시 패턴도 프레임워크가 서비스 메시 컨트롤 플레인 설정 API를 지원해야하고, <br>
클라이언트 라이브러리 자체가 네트워크 통신 로직을 구현해야한다는 복잡한 문제는 있음


지금까지는 클라우드 네이티브 애플리케이션의 연결성을 위해서 쿠버네티스 같은 플랫폼의 서비스 추상화나 레지스트리 및 검색, 사이드카 같은 패턴을 알아보았음
<img width="563" height="782" alt="Screenshot 2025-09-25 at 12 02 09 PM" src="https://github.com/user-attachments/assets/e327b842-b315-40ef-8b9a-90fa0b5f3587" />


---

<br>


## 서비스 조합 패턴

- 클라우드 네이티브 애플리케이션의 비즈니스 기능은 여러 마이크로서비스 간의 상호작용을 통해 완성됨
- 이전까지는 어떻게 서비스와 시스템의 연결하는지 알아보았다면
- 이제부터는 서비스와 비즈니스 로직을 파악하고 조합하는 방법을 알아봄

### 서비스 오케스트레이션 패턴

- 여러 마이크로서비스 시스템들을 통합하고 호출해서 비즈니스 로직을 구현하는 것
- 다운스트림 서비스에서는 동기/비동기 통신을 통해 다른 서비스를 호출하는데, 이를 서비스 체인으로 부른다.

<img width="555" height="494" alt="Screenshot 2025-09-25 at 12 02 28 PM" src="https://github.com/user-attachments/assets/889119e2-4a11-4b72-bc2a-530dbba53eec" />

- 오케스트레이션 라이브러리/ 혹은 직접 구현한 사례?

- 서비스 오케스트레이션을 적용할 때는
    - 여러 다운스트림 기능을 조합하여 특정 비즈니스 기능을 만들 수 있는 경우에만 적용해야함(아니면 그냥 여러 비즈니스 기능을 가지는 모놀리식 서비스에 불과)
    - 오케스트레이션 조합 로직은 하나의 서비스에서만 처리하며, 이 서비스는 다른 모든 서비스에 강한 의존성을 가지게 됨
    

### 코레오그라피 패턴

- 마이크로서비스 시스템 간 비동기 통신을 통해 서비스를 조합하고 비즈니스 기능을 만드는 패턴으로,
    - 메시지 브로커나 이벤트 허브를 통한 비동기 이벤트 주도 통신 방법을 사용
    - 마이크로 서비스가 직접 마이크로 서비스를 호출하지 않으며, 유입되는 이벤트와 메시지를 통해 외부 서비스 호출에 반응하는 구조
 
<img width="508" height="185" alt="Screenshot 2025-09-25 at 12 02 42 PM" src="https://github.com/user-attachments/assets/afc0c283-9a6f-486d-8e70-aa8c2e93e502" />


- 해당 패턴은 큐나 토픽에 메시지를 발행하고, 이 메시지를 구독하여 소비하는 마이크로 서비스들로 구성됨
    - 큐 기반 단일 수신자나 pub-sub 구조와 같은 다중 수신자 패턴으로 비동기 조합 로직을 분산하여 구현할 수 있음
- 마이크로 서비스 A가 이벤트를 받아서 메시지를 발행 → 브로커 큐에 들어간 메시지를 C나 D가 소비하여 각자 처리하고 → 또 그 결과를 메시지로 생성해서 브로커나 큐에 다시 전달하는 방식으로 반응함.
- 기본적으로 코레오그라피 패턴은 메시지 브로커를 기본 메시징 인프라스트럭처로 사용하기 때문에, 브로커에는 비즈니스 로직을 구현하면 안됨

- 서비스 코레오그라피 패턴은 메시지 브로커를 통한 비동기 메시징으로 서비스를 잘 조작하기만 하면 됨.

<img width="501" height="213" alt="Screenshot 2025-09-25 at 12 02 52 PM" src="https://github.com/user-attachments/assets/165becd1-175e-49c3-a11f-f1d436685a4c" />

- 주문 서비스 → 결제 서비스 → 배송
- 배송 추적에 따라 배송현황, 주문 상태 변경 서비스가 각자 메시지를 소비하여 상태 처리


### 사가 패턴

- 여러 마이크로 서비스를 묶어서 새 서비스 조합을 만들 때, 서비스 간 상호작용을 트랜잭션으로 묶어서 동작시켜야하는 경우가 있음
    - 한 서비스에서 동작이 실패하면 → 다른 서비스는 취소하는 방식
    - 분산 트랜잭션이라고도 부름

- 여러 하위 트랜잭션 및 연계되는 보상 트랜잭션으로 나누어서 마이크로서비스들과 시스템에 배치함으로써 분산 트랜잭션을 만듦
    - 사가 패턴의 모든 트랜잭션은 전체가 성공해야 트랜잭션이 완성(Do or Not)
    - 하나라도 실패하면 보상 트랜잭션을 실행해 전체 하위 트랜잭션을 취소하고 원래 상태로 복구한다.
    - 주로 2PC와 비교되는데, 2PC의 경우 트랜잭션 관리자가 SPOF가 되기도 하며 특정 서비스가 응답하지 않을 시 전체 트랜잭션이 멈추어버린다는 문제가 있음
    - 그래서 마이크로 서비스들이 분산되어 각자 동작하는 환경 특성 상 분산 트랜잭션과 2PC 프로토콜로 구현하는 것은 복잡하여 잘 사용되지 않는다.
 
<img width="519" height="201" alt="Screenshot 2025-09-25 at 12 03 22 PM" src="https://github.com/user-attachments/assets/f74f61e2-0fda-44ae-bd51-d52da6b532eb" />

- 사가 패턴은 분산 트랜잭션을 여러 로컬 트랜잭션으로 나누고
- 각 로컬 트랜잭션을 취소하고 상태를 원복할 보상 트랜잭션과 함께 묶어서 배치함
- T1 트랜잭션이 실패 → C1 트랜잭션을 수행하여 다시 복구하는 방식
- 마이크로 서비스 X의 경우는 사가 실행 관리자이며(SEC), 이는 BPMN과 같은 중앙집중형 워크플로 솔루션과 같은 개념임
- 하지만 클라우드 네이티브 애플리케이션에서는 마이크로서비스들도 분산되어있고  마이크로서비스들이 추가/제거되기 쉽기 때문에 분산 트랜잭션의 보관기간이 짧아
- 클라우드 네이티브 애플리케이션에서 사가 패턴을 구현할 때는 saga log를 통해 트랜잭션을 기록하고, 별도로 보관해야함
    - 사가 로그는 SEC 컴포넌트가 관리해야함
- 사가는 주로 오케스트레이션 패턴과 함께 사용


<img width="528" height="169" alt="Screenshot 2025-09-25 at 12 03 35 PM" src="https://github.com/user-attachments/assets/74ebf9b6-3da4-414e-b10d-ee67fb02253a" />

- 사가 패턴을 적용하면 항공권 예약을 할 때 하나의 트랜잭션이 취소되면 다른 트랜잭션들도 취소되어야함
- 여행예약 서비스는 분산 로그를 사용하여 각 트랜잭션들을 기록하고 비즈니스 로직을 구현한다.
- Camunda / 아파치 카멜 등의 프레임워크가 지원

- 사가 패턴은 필요한 경우에만 사용하는 것이 바람직하며,
    - 밑바닥부터 구현하는 것은 복잡하고 규모가 크므로 프레임워크나 워크플로 엔진을 권장
    - 비즈니스 트랜잭션을 디버깅하고 문제를 해결하려면 관측 가능한 솔루션이 필요함
    - 확장 가능한 분산로그도 필수


<img width="545" height="342" alt="Screenshot 2025-09-25 at 12 04 07 PM" src="https://github.com/user-attachments/assets/2f107df1-08b9-496f-b3a1-4009b16547b8" />







